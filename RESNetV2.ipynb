{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c38f1ba-c2a2-472d-999d-fc3d2c4e4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import random \n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import Utils\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.metrics as metrics\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet101V2, ResNet152V2, ResNet50V2\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB5\n",
    "\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6122fe12-51a6-42a1-9996-69ddde587370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tf_gpu():\n",
    "    # Reduce logging output.\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    # dynamically grow the memory used on the GPU\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # Allowing TF to automatically choose an existing and\n",
    "    # supported device to run the operations in case the specified one doesn't exist\n",
    "    config.allow_soft_placement = True\n",
    "    # to log device placement (on which device the operation ran)\n",
    "    config.log_device_placement = False\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    # set this TensorFlow session as the default session for Keras\n",
    "    K.set_session(sess)\n",
    "\n",
    "def build_model(shape):\n",
    "    base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=shape, pooling='max')\n",
    "    base_model.trainable=False\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)   \n",
    "    model.add(layers.BatchNormalization(axis = -1 , momentum = 0.99 , epsilon = 0.001))\n",
    "    model.add(layers.Dense(256, kernel_regularizer = regularizers.l2(l=0.016) , activity_regularizer=regularizers.l1(0.006),\n",
    "         bias_regularizer= regularizers.l1(0.006) , activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adamax(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def prepare_images(root_path, seed, test_size=0.3):\n",
    "    if os.path.isdir(root_path + '/train/cat') or os.path.isdir(root_path + 'test/cat'):\n",
    "        shape = Image.open(glob.glob(root_path + '/train/cat/*.jpg')[0]).size\n",
    "        return root_path + '/train', root_path + '/test', shape\n",
    "    os.makedirs(root_path + '/train/cat')\n",
    "    os.makedirs(root_path + '/train/dog')\n",
    "    os.makedirs(root_path + '/test/cat')\n",
    "    os.makedirs(root_path + '/test/dog')\n",
    "    files = glob.glob(root_path + '/*.jpg')\n",
    "    shape = Image.open(files[0]).size\n",
    "    \n",
    "    N = len(files)\n",
    "    n_train = int((1-test_size)*N)\n",
    "    random.seed(42)\n",
    "    population = range(N)\n",
    "    train_indices = random.sample(population, n_train)\n",
    "    test_indices = list(set(population).difference(set(train_indices)))\n",
    "    ## train images\n",
    "    for i in train_indices:\n",
    "        file = files[i]\n",
    "        if 'cat' in file:\n",
    "            shutil.move(file, root_path + '/train/cat/' + str(i) + '.jpg')\n",
    "        if 'dog' in file:\n",
    "            shutil.move(file, root_path + '/train/dog/' + str(i) + '.jpg')\n",
    "    ## test images\n",
    "    for i in test_indices:\n",
    "        file = files[i]\n",
    "        if 'cat' in file:\n",
    "            shutil.move(file, root_path + '/test/cat/' + str(i) + '.jpg')\n",
    "        if 'dog' in file:\n",
    "            shutil.move(file, root_path + '/test/dog/' + str(i) + '.jpg')   \n",
    "    return root_path + '/train', root_path + '/test', shape\n",
    "\n",
    "def unzip_images(zip_path):\n",
    "    if not os.path.isfile(zip_path):\n",
    "        return -1\n",
    "    root_dir = str(Path(zip_path).parent)\n",
    "    if os.path.isdir(root_dir + '/Image_set/'):\n",
    "        return root_dir + '/Image_set/train/'\n",
    "    extract_path = root_dir + '/Image_Set/'\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(root_dir + '/Image_Set/')\n",
    "    if os.path.isfile(extract_path + 'train.zip'):\n",
    "        with zipfile.ZipFile(extract_path + 'train.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "    return extract_path + '/train/'        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f59d55-a8d5-486d-85c2-012a492f35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download Images\n",
    "zip_path = 'dogs-vs-cats.zip'\n",
    "if not os.path.isfile(zip_path):\n",
    "    Utils.download_file_from_google_drive(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3b6fad-f87b-4a15-a204-faf6ec66b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract zip files and prepare images\n",
    "root_dir = unzip_images(zip_path)\n",
    "seed=42\n",
    "img_size = (224 , 224, 3)\n",
    "train_path, test_path, shape = prepare_images(root_dir, seed=seed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea39cc5-252c-4382-bd88-190a86023fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb5 (Functional)  (None, 2048)             28513527  \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2048)             8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,046,520\n",
      "Trainable params: 528,897\n",
      "Non-trainable params: 28,517,623\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(img_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f61806c-d80e-4224-906a-83fb47fbc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18001 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "## Training Data\n",
    "train_gen = ImageDataGenerator(rescale=1/255, validation_split=0.1)\n",
    "train_set = train_gen.flow_from_directory(train_path, target_size=img_size[:-1], batch_size=batch_size, shuffle=True, \n",
    "                                          seed=seed, subset='training', class_mode='binary')\n",
    "# Validation data\n",
    "val_set = train_gen.flow_from_directory(train_path, target_size=img_size[:-1], batch_size=batch_size, shuffle=True, \n",
    "                                        seed=seed, subset='validation', class_mode='binary')\n",
    "## test data\n",
    "test_gen = ImageDataGenerator(rescale=1/255)\n",
    "test_set = test_gen.flow_from_directory(test_path, target_size=img_size[:-1], batch_size=batch_size, shuffle=True, \n",
    "                                        seed=seed, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0972c39-5d32-4cb6-99f5-e7a284dca3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "train_frac = 0.1 ## fraction of the train set used in each optimization step\n",
    "spe = (train_frac * train_set.n) // batch_size\n",
    "\n",
    "os.makedirs('Model/', exist_ok=True)\n",
    "callback_1 = ModelCheckpoint('Model/', save_freq=int(spe), save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c958a9da-2c43-43cc-9490-5e1fa799757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus):\n",
    "    #tf.config.set_visible_devices([], 'CPU') # hide the CPU\n",
    "    #tf.config.set_visible_devices(gpus[0], 'GPU') # unhide potentially hidden GPU\n",
    "    #tf.config.get_visible_devices()\n",
    "    configure_tf_gpu()\n",
    "    device = tf.config.list_logical_devices('GPU')[0]\n",
    "else:\n",
    "    device = tf.config.list_logical_devices('CPU')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35e90daf-5a61-4104-887c-c6ac4379b169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/56 [============================>.] - ETA: 0s - loss: 5.0310 - accuracy: 0.5398"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__SaveV2_dtypes_765_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to WriteFile: Model\\_temp/part-00000-of-00001.data-00000-of-00001.tempstate5916014653102938877 : Espacio en disco insuficiente.\r\n; operation in progress [Op:SaveV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#with tf.device('/device:GPU:0'):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(device\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback_1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jesus\\desktop\\catvsdog\\catvsdog_ml\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\jesus\\desktop\\catvsdog\\catvsdog_ml\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__SaveV2_dtypes_765_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to WriteFile: Model\\_temp/part-00000-of-00001.data-00000-of-00001.tempstate5916014653102938877 : Espacio en disco insuficiente.\r\n; operation in progress [Op:SaveV2]"
     ]
    }
   ],
   "source": [
    "#with tf.device('/device:GPU:0'):\n",
    "with tf.device(device.name):\n",
    "    model.fit(train_set, epochs=20, callbacks=[callback_1], steps_per_epoch=spe, validation_data=val_set, validation_steps=2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484486e-7c00-453a-95bd-bce977cc8683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
